{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c154e",
   "metadata": {
    "id": "635c154e"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import anthropic\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1ed4a",
   "metadata": {
    "id": "03f1ed4a"
   },
   "outputs": [],
   "source": [
    "GROK_KEY = \"YOUR_KEY\"\n",
    "CHAT_GPT_KEY = \"YOUR_KEY\"\n",
    "GEMINI_API_KEY = \"YOUR_KEY\"\n",
    "CLAUDE_KEY = \"YOUR_KEY\"\n",
    "\n",
    "ANSWERS_JSON = \"answers.json\"\n",
    "FINAL_RESULTS_JSON = \"gemini_judge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a540dd2",
   "metadata": {
    "id": "2a540dd2"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "DATASET_DIR = \"dataset\"\n",
    "\n",
    "youtube_data = []\n",
    "\n",
    "for fname in os.listdir(DATASET_DIR):\n",
    "    if fname.endswith(\".json\"):\n",
    "        with open(os.path.join(DATASET_DIR, fname), encoding=\"utf-8\") as f:\n",
    "            youtube_data.append(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24917509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\LLM4EDU\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.11) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\Development\\LLM4EDU\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Vinh\\AppData\\Local\\Temp\\ipykernel_16592\\2443543213.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import regex as re\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c410e13",
   "metadata": {
    "id": "6c410e13",
    "outputId": "66c5a6e7-ee46-403e-eb35-cf2b5d79f0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 rows from ../answers/answers.csv. Starting to call gemini for each row...\n",
      "Processing (1/80) ...\n",
      "\n",
      "I'm doing scientific research, I'll describe the research, please help me be a judge to grade it. Description:\n",
      "Objective. This project aims to evaluate the performance of large language models (LLMs) in responding to real-world queries based on YouTube comments, particularly in the context of online educational videos.\n",
      "Context. Viewer comments on YouTube not only express emotions but also reflect learners’ understanding, interests, and learning experiences. Querying and synthesizing information from these comments can help educators and content managers better capture learner feedback and improve instructional quality.\n",
      "Data. The queries were constructed based on user comments from 9 videos in the Stanford CS230: Deep Learning (Autumn 2025), \n",
      "Methodology. The study uses a set of 80 queries representing diverse tasks such as: Information extraction, Sentiment analysis, Topic identification, Inference, Summarization and judgment, etc.\n",
      "The three LLMs being compared are:\n",
      "ChatGPT 5.1, Like K2, and Grok 4.1 (however, to ensure fairness, I will hide the names of the responding models and replace them with random A, B, C)\n",
      "Each model's responses are evaluated according to three key criteria:\n",
      "Accuracy - Correctness of information and avoidance of unsupported hallucinations\n",
      "Relevance - Alignment with the query's intent and requirements\n",
      "Coverage - Breadth and completeness in addressing key aspects mentioned or implied\n",
      "✍️ How to Evaluate\n",
      "Each response will be evaluated using two methods:\n",
      "Pairwise Comparison: Choose the better response between two models\n",
      "Pointwise scoring: Rate each model independently on a scale from 1 to 10\n",
      "Now I will provide comments on each of the 9 videos. You can start when you have commented on all 9 and I signal you to rate them.\n",
      "—--------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now let's begin the scoring. Based on the detailed data from the 9 videos I've provided, please act as the judge and score the following 12 questions:\n",
      "Part 1: Pair Matching - Choose the better response between two models\n",
      "Question 1: Which deep learning concepts are most frequently mentioned in the comments?\n",
      "Answer A [Based on the full comment set, the most frequently mentioned deep-learning concepts are:\n",
      "1. Supervised / Self-Supervised / Weakly Supervised Learning\n",
      "Explicitly referenced in Lecture 2; many viewers ask where to find related videos, slides, and examples.\n",
      "2. Adversarial Robustness & Adversarial Machine Learning\n",
      "From Lecture 4, several commenters discuss adversarial behaviors, high-dimensional decision boundaries, and security concerns.\n",
      "3. Generative Models\n",
      "Especially GAN-related notions and general generative modeling appear often in Lecture 4 comments.\n",
      "4. Reinforcement Learning (RL) & Deep Q-Learning (DQN)\n",
      "Lecture 5 has heavy comment activity about Q-learning, Bellman equations, DQN architecture, exploration vs exploitation, Atari examples, and RLHF.\n",
      "5. RLHF (Reinforcement Learning from Human Feedback)\n",
      "Repeated references to reward models, preference data, and ChatGPT-style training.\n",
      "6. Agents, Prompt Engineering, and RAG\n",
      "Lecture 8 generates many mentions of:\n",
      "prompt engineering (CoT, prompt chaining),\n",
      "RAG workflows (embeddings, vector DBs),\n",
      "agentic AI,\n",
      "Model Context Protocol (MCP).\n",
      "7. Full-cycle DL project workflow\n",
      "Comments call out strategies for dataset design, training loops, debugging, and production considerations (Lecture 3).\n",
      "8. Model interpretability / “What’s going on inside my model?”\n",
      "From Lecture 10, several comments mention data tracing and understanding internals.\n",
      "Most dominant themes across all lectures:\n",
      "RL (especially DQN & RLHF)\n",
      "Agents + Prompt Engineering + RAG\n",
      "Supervised vs. self-supervised learning\n",
      "Adversarial robustness\n",
      "If you want, I can also produce a frequency-ranked table or cluster these themes by lecture.];\n",
      "Answer B [Top deep learning concepts mentioned in comments across all lectures: \n",
      "\n",
      "1. RAG (Retrieval-Augmented Generation) – Frequently referenced in Lecture 8 comments (e.g., questions about RAG techniques, HyDE, chunking). \n",
      "2. Agents / Agentic Workflows – Heavily discussed in Lecture 8 (multi-agent systems, autonomy levels, MCP vs APIs, tool use). \n",
      "3. Prompt Engineering / Prompting – Common in Lecture 8 (chain-of-thought, few-shot/zero-shot, prompt chaining, evaluation). \n",
      "4. Fine-tuning – Mentioned multiple times (limitations, overfitting, comparison to prompting/RAG). \n",
      "5. RLHF (Reinforcement Learning from Human Feedback) – Highlighted in Lecture 5 comments and timestamps. \n",
      "6. Reinforcement Learning / Deep RL – Appears in Lecture 5 (DQN, PPO, AlphaGo, self-play). \n",
      "7. Adversarial (Robustness/Attacks) – Noted in Lecture 4 comments. \n",
      "\n",
      "Other occasional mentions: self-supervised learning, generative models, embeddings/vector DBs, interpretability (Lecture 10 title). \n",
      "\n",
      "Lecture 8 (Agents, Prompts, RAG) dominates recent discussions, driving most mentions of modern LLM-related concepts.];\n",
      "Answer C:[The most frequently mentioned deep-learning concepts in the comments are:\n",
      "\n",
      "1. Reinforcement Learning / Deep Q-Learning \n",
      "2. RAG (Retrieval-Augmented Generation) \n",
      "3. Agents / Agentic Workflows \n",
      "4. Prompt Engineering / Chain-of-Thought \n",
      "5. Fine-tuning vs. RAG trade-off].\n",
      "Accuracy\n",
      "Accuracy assesses the extent to which answers accurately reflect the information contained in the comments, avoiding false information, unfounded speculation, and fabricated content not supported by comment data.\n",
      "1. Based on the two answers to question 1, if considering accuracy alone, which answer, A or B, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "2. Based on the two answers to question 1, if considering accuracy alone, which answer, A or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ A \n",
      "☐ C \n",
      "☐ A and C are tied\n",
      "3. Based on the two answers to question 1, considering accuracy alone, which answer, B or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "Relevance\n",
      "Relevance assesses the degree to which an answer closely addresses the query's requirements. An answer is considered highly relevant if it directly addresses the core question, is concise, doesn't stray from the topic, and doesn't omit any key elements of the query.\n",
      "Based on the two answers to question 1, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
      "4. Based on the two answers to question 1, considering only their relevance, which answer, A or B, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "5. Based on the two answers to question 1, considering only their relevance, which answer, A or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ A \n",
      "☐ C\n",
      "☐ A and C are tied\n",
      "6. Based on the two answers to question 1, considering only their relevance, which answer, B or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "3. Coverage\n",
      "Coverage assesses the extent to which an answer fully addresses the important aspects requested in the query. A highly comprehensive answer will synthesize multiple dimensions of information contained in the comments, without omitting any key points or necessary aspects.\n",
      "Based on the two answers to question 1, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
      "7. Based on the two answers to question 1, considering only the coverage, which answer, A or B, provides a more complete picture of the key aspects of the query?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "8. Based on the two answers to question 1, considering only the coverage, which answer, A or C, provides a more complete picture of the key aspects of the query?\n",
      "☐ A \n",
      "☐ C\n",
      "☐ A and C are tied\n",
      "9. Based on the two answers to question 1, considering only the coverage, which answer, B or C, provides a more complete picture of the key aspects of the query?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "Part 2: Scoring on a 10-point scale\n",
      "Accuracy\n",
      "10. To what extent do the answers accurately reflect the information contained in the comments, and do they avoid misinformation or unfounded speculation?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Serious discrepancies:The answer contains largely false, fabricated (hallucination), or directly contradictory information that contradicts the input data. It is harmful to or misleads the user.\n",
      "3 – 4\n",
      "Weak:While some information is correct, it is mixed with a lot of misinformation or unfounded speculation. Users cannot trust this answer without verifying it.\n",
      "5 – 6\n",
      "Medium:The basic information is correct, but there are some minor errors in figures, proper names, or supplementary details. There are no serious errors in terms of logic/background knowledge.\n",
      "7 – 8\n",
      "Good:The information is accurate and reliable. There are no factual errors. Any inferences (if any) are logically based on the data.\n",
      "9 – 10\n",
      "Excellent:Absolutely 100% accurate. Every statement is true. There is no ambiguity whatsoever regarding its correctness.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Relevance\n",
      "11. To what extent does the answer closely address the query's requirements? Does it stay focused, avoiding rambling or omitting key points?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Digress:The answer is irrelevant to the question or addresses a different question altogether. Completely useless.\n",
      "3 – 4\n",
      "Weak:The topic is mentioned but the response is roundabout, contains too much unnecessary information (filler words), or repeats the question verbatim without addressing the issue.\n",
      "5 – 6\n",
      "Medium:The answers are reasonably focused, but there are still rambling passages or disorganized sentence structures that make it difficult for the reader to grasp the main points.\n",
      "7 – 8\n",
      "Good:It directly answers the question. The structure is clear and easy to understand. It eliminates most of the distracting information.\n",
      "9 – 10\n",
      "Excellent:The answer is concise, brief, yet valuable. It gets straight to the point from the first sentence. The formatting (bullet point, bold/light) is optimized for readability.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Relevance\n",
      "12. To what extent does the answer address all the important aspects raised in the query? Are there any significant points omitted?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Very flawed:Ignoring most of the main requirements of the question. Only answering a very small or insignificant part of it.\n",
      "3 – 4\n",
      "Lack:Omitting at least one important/core aspect of the question. For example: Asking about advantages and disadvantages but only mentioning the advantages.\n",
      "5 – 6\n",
      "Medium:The main points are mentioned, but the discussion lacks depth and is superficial. It is missing necessary supporting details or illustrative examples.\n",
      "7 – 8\n",
      "Good:The response addresses all aspects of the query, leaving no key points unanswered. The depth of the response is good.\n",
      "9 – 10\n",
      "Excellent:Comprehensive and thorough. It not only answers all the questions but also provides context, insightful perspectives, or exceptions (if needed). Exceeds expectations in terms of detail.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Please help me answer the following 12 questions.\n",
      "Processing (2/80) ...\n",
      "\n",
      "I'm doing scientific research, I'll describe the research, please help me be a judge to grade it. Description:\n",
      "Objective. This project aims to evaluate the performance of large language models (LLMs) in responding to real-world queries based on YouTube comments, particularly in the context of online educational videos.\n",
      "Context. Viewer comments on YouTube not only express emotions but also reflect learners’ understanding, interests, and learning experiences. Querying and synthesizing information from these comments can help educators and content managers better capture learner feedback and improve instructional quality.\n",
      "Data. The queries were constructed based on user comments from 9 videos in the Stanford CS230: Deep Learning (Autumn 2025), \n",
      "Methodology. The study uses a set of 80 queries representing diverse tasks such as: Information extraction, Sentiment analysis, Topic identification, Inference, Summarization and judgment, etc.\n",
      "The three LLMs being compared are:\n",
      "ChatGPT 5.1, Like K2, and Grok 4.1 (however, to ensure fairness, I will hide the names of the responding models and replace them with random A, B, C)\n",
      "Each model's responses are evaluated according to three key criteria:\n",
      "Accuracy - Correctness of information and avoidance of unsupported hallucinations\n",
      "Relevance - Alignment with the query's intent and requirements\n",
      "Coverage - Breadth and completeness in addressing key aspects mentioned or implied\n",
      "✍️ How to Evaluate\n",
      "Each response will be evaluated using two methods:\n",
      "Pairwise Comparison: Choose the better response between two models\n",
      "Pointwise scoring: Rate each model independently on a scale from 1 to 10\n",
      "Now I will provide comments on each of the 9 videos. You can start when you have commented on all 9 and I signal you to rate them.\n",
      "—--------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now let's begin the scoring. Based on the detailed data from the 9 videos I've provided, please act as the judge and score the following 12 questions:\n",
      "Part 1: Pair Matching - Choose the better response between two models\n",
      "Question 2: Which algorithms or models are commonly mentioned by viewers?\n",
      "Answer A [From all viewer comments across the 10 lectures, the commonly mentioned algorithms and models are:\n",
      "1. Deep Q-Network (DQN)\n",
      "Referenced repeatedly in Lecture 5 timestamps and discussions (Q-tables, Bellman equation, Atari games).\n",
      "2. RLHF Components\n",
      "Not a single algorithm but commonly mentioned elements:\n",
      "Reward Models\n",
      "Supervised Fine-Tuning (SFT)\n",
      "Policy optimization (general RL formulations)\n",
      "3. PPO / TRPO / DPO\n",
      "Explicitly listed in Lecture 5 viewer summaries as “other RL algorithms.”\n",
      "4. Atari / AlphaGo / AlphaStar / Dota2 agents\n",
      "These aren’t algorithms themselves but refer to well-known RL systems using deep RL methods.\n",
      "5. CNN-based architectures (implied through DQN)\n",
      "Mentioned as part of preprocessing and architecture in RL.\n",
      "6. Generative Models\n",
      "Broadly referenced in Lecture 4 comments (adversarial robustness + generative modeling), though specific model names are not listed.\n",
      "7. RAG (Retrieval-Augmented Generation)\n",
      "Not a classic algorithm but a commonly referenced architecture pattern.\n",
      "8. Prompt Engineering & CoT (Chain-of-Thought)\n",
      "Again conceptual methods rather than classical algorithms but repeatedly mentioned.\n",
      "9. MCP (Model Context Protocol)\n",
      "A system-level model/tooling interface mentioned heavily in Lecture 8 discussions.\n",
      "10. LLMs (general)\n",
      "Models like GPT-4o, o3, Grok, and “base models” are mentioned but not explored algorithmically in comments.\n",
      "\n",
      "Most explicitly named algorithms:\n",
      "DQN\n",
      "Bellman equation (Q-learning target)\n",
      "PPO / TRPO / DPO\n",
      "If you'd like, I can also extract direct viewer quotes referencing each algorithm.];\n",
      "Answer B [Commonly mentioned algorithms/models in viewer comments: \n",
      "\n",
      "1. DQN (Deep Q-Network) – Lecture 5 (Atari, experience replay, ε-greedy). \n",
      "2. PPO (Proximal Policy Optimization) – Lecture 5 (continuous control, mentioned alongside TRPO/DPO). \n",
      "3. RLHF – Lecture 5 (bridge from GPT-2 to ChatGPT, reward models). \n",
      "4. AlphaGo / AlphaStar – Lecture 5 (self-play, emergent strategies). \n",
      "5. RAG – Lecture 8 (chunking, HyDE, vector DBs). \n",
      "6. Fine-tuning / SFT (Supervised Fine-Tuning) – Lectures 3, 8 (limitations, overfitting vs prompting). \n",
      "7. Chain-of-Thought (CoT) – Lecture 8 (prompting technique). \n",
      "\n",
      "Other occasional: Bellman equation, embeddings, MCP (Model Context Protocol), ReAct prompting. Lecture 5 (RL) and Lecture 8 (modern LLM techniques) drive most specific mentions.];\n",
      "Answer C:[Most-named algorithms/models in comments: \n",
      "- Deep Q-Network (DQN) \n",
      "- PPO / TRPO / DPO \n",
      "- GPT-3.5 / GPT-4 / ChatGPT \n",
      "- AlphaGo / AlphaStar \n",
      "- CNN (for Atari DQN)].\n",
      "Accuracy\n",
      "Accuracy assesses the extent to which answers accurately reflect the information contained in the comments, avoiding false information, unfounded speculation, and fabricated content not supported by comment data.\n",
      "1. Based on the two answers to question 2, if considering accuracy alone, which answer, A or B, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "2. Based on the two answers to question 2, if considering accuracy alone, which answer, A or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ A \n",
      "☐ C \n",
      "☐ A and C are tied\n",
      "3. Based on the two answers to question 2, considering accuracy alone, which answer, B or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "Relevance\n",
      "Relevance assesses the degree to which an answer closely addresses the query's requirements. An answer is considered highly relevant if it directly addresses the core question, is concise, doesn't stray from the topic, and doesn't omit any key elements of the query.\n",
      "Based on the two answers to question 2, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
      "4. Based on the two answers to question 2, considering only their relevance, which answer, A or B, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "5. Based on the two answers to question 2, considering only their relevance, which answer, A or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ A \n",
      "☐ C\n",
      "☐ A and C are tied\n",
      "6. Based on the two answers to question 2, considering only their relevance, which answer, B or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "3. Coverage\n",
      "Coverage assesses the extent to which an answer fully addresses the important aspects requested in the query. A highly comprehensive answer will synthesize multiple dimensions of information contained in the comments, without omitting any key points or necessary aspects.\n",
      "Based on the two answers to question 2, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
      "7. Based on the two answers to question 2, considering only the coverage, which answer, A or B, provides a more complete picture of the key aspects of the query?\n",
      "☐ A \n",
      "☐ B \n",
      "☐ A and B are tied\n",
      "8. Based on the two answers to question 2, considering only the coverage, which answer, A or C, provides a more complete picture of the key aspects of the query?\n",
      "☐ A \n",
      "☐ C\n",
      "☐ A and C are tied\n",
      "9. Based on the two answers to question 2, considering only the coverage, which answer, B or C, provides a more complete picture of the key aspects of the query?\n",
      "☐ B \n",
      "☐ C\n",
      "☐ B and C are tied\n",
      "Part 2: Scoring on a 10-point scale\n",
      "Accuracy\n",
      "10. To what extent do the answers accurately reflect the information contained in the comments, and do they avoid misinformation or unfounded speculation?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Serious discrepancies:The answer contains largely false, fabricated (hallucination), or directly contradictory information that contradicts the input data. It is harmful to or misleads the user.\n",
      "3 – 4\n",
      "Weak:While some information is correct, it is mixed with a lot of misinformation or unfounded speculation. Users cannot trust this answer without verifying it.\n",
      "5 – 6\n",
      "Medium:The basic information is correct, but there are some minor errors in figures, proper names, or supplementary details. There are no serious errors in terms of logic/background knowledge.\n",
      "7 – 8\n",
      "Good:The information is accurate and reliable. There are no factual errors. Any inferences (if any) are logically based on the data.\n",
      "9 – 10\n",
      "Excellent:Absolutely 100% accurate. Every statement is true. There is no ambiguity whatsoever regarding its correctness.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Relevance\n",
      "11. To what extent does the answer closely address the query's requirements? Does it stay focused, avoiding rambling or omitting key points?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Digress:The answer is irrelevant to the question or addresses a different question altogether. Completely useless.\n",
      "3 – 4\n",
      "Weak:The topic is mentioned but the response is roundabout, contains too much unnecessary information (filler words), or repeats the question verbatim without addressing the issue.\n",
      "5 – 6\n",
      "Medium:The answers are reasonably focused, but there are still rambling passages or disorganized sentence structures that make it difficult for the reader to grasp the main points.\n",
      "7 – 8\n",
      "Good:It directly answers the question. The structure is clear and easy to understand. It eliminates most of the distracting information.\n",
      "9 – 10\n",
      "Excellent:The answer is concise, brief, yet valuable. It gets straight to the point from the first sentence. The formatting (bullet point, bold/light) is optimized for readability.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Relevance\n",
      "12. To what extent does the answer address all the important aspects raised in the query? Are there any significant points omitted?\n",
      "Score, Description of criteria\n",
      "1 – 2\n",
      "Very flawed:Ignoring most of the main requirements of the question. Only answering a very small or insignificant part of it.\n",
      "3 – 4\n",
      "Lack:Omitting at least one important/core aspect of the question. For example: Asking about advantages and disadvantages but only mentioning the advantages.\n",
      "5 – 6\n",
      "Medium:The main points are mentioned, but the discussion lacks depth and is superficial. It is missing necessary supporting details or illustrative examples.\n",
      "7 – 8\n",
      "Good:The response addresses all aspects of the query, leaving no key points unanswered. The depth of the response is good.\n",
      "9 – 10\n",
      "Excellent:Comprehensive and thorough. It not only answers all the questions but also provides context, insightful perspectives, or exceptions (if needed). Exceeds expectations in terms of detail.\n",
      "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
      "Please help me answer the following 12 questions.\n",
      "Done. Processed 2 rows. Results saved to ../results/example_final_results.json\n"
     ]
    }
   ],
   "source": [
    "FINAL_RESULTS_JSON = \"../results/example_final_results.json\"\n",
    "CSV_PATH = \"../answers/answers.csv\"\n",
    "\n",
    "def parse_json_safe(text):\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group()\n",
    "            return json.loads(json_str)\n",
    "        return {\"error\": \"Không tìm thấy cấu trúc JSON\", \"raw\": text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Lỗi parse: {str(e)}\", \"raw\": text}\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = text.replace(\"**\", \"\")\n",
    "    text = re.sub(r'#+\\s', '', text)\n",
    "    text = re.sub(r'^\\s*[\\*\\-]\\s+', '', text, flags=re.MULTILINE)\n",
    "    return text.strip()\n",
    "\n",
    "def call_gemini(prompt_text):\n",
    "    while True:\n",
    "        try:\n",
    "            genai.configure(api_key = GEMINI_API_KEY)\n",
    "            # replace 'gemini-3-flash-preview' with the desired Gemini model name\n",
    "            model = genai.GenerativeModel('gemini-3-pro-preview')\n",
    "            response = model.generate_content(prompt_text)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                print(\"Wait 20s ....\")\n",
    "                time.sleep(20)\n",
    "                continue\n",
    "            return f\"Error Gemini Judge: {e}\"\n",
    "\n",
    "def call_gpt(prompt_text):\n",
    "    try:\n",
    "        client = OpenAI(api_key = CHAT_GPT_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error ChatGPT: {e}\"\n",
    "\n",
    "def call_grok(prompt_text):\n",
    "    try:\n",
    "        client = OpenAI(api_key = GROK_KEY,\n",
    "                        base_url=\"https://api.x.ai/v1\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"grok-4-1-fast-reasoning\",\n",
    "            messages=[{\"role\": \"user\",\n",
    "                       \"content\": prompt_text}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error Grok: {e}\"\n",
    "\n",
    "def call_claude(prompt_text):\n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=CLAUDE_KEY)\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error Claude: {e}\"\n",
    "\n",
    "def save_json_data(filename, new_entry):\n",
    "    data = []\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except:\n",
    "                data = []\n",
    "    data.append(new_entry)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "def append_json(path, obj):\n",
    "    \"\"\"Append an object to a JSON list file (tạo file nếu chưa có).\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    data = []\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if not isinstance(data, list):\n",
    "                    data = []\n",
    "        except Exception:\n",
    "            data = []\n",
    "    data.append(obj)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def _get_field(row, candidates):\n",
    "    \"\"\"Trả về giá trị trường trong row theo danh sách tên khả dĩ.\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in row and row[k] is not None:\n",
    "            return row[k].strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_judge_prompt(number, question_text, answer_a, answer_b, answer_c):\n",
    "    \"\"\"Xây prompt giữ nguyên phần mô tả, chỉ thay number, question_text, answer_a/b/c.\"\"\"\n",
    "    return f\"\"\"\n",
    "I'm doing scientific research, I'll describe the research, please help me be a judge to grade it. Description:\n",
    "Objective. This project aims to evaluate the performance of large language models (LLMs) in responding to real-world queries based on YouTube comments, particularly in the context of online educational videos.\n",
    "Context. Viewer comments on YouTube not only express emotions but also reflect learners’ understanding, interests, and learning experiences. Querying and synthesizing information from these comments can help educators and content managers better capture learner feedback and improve instructional quality.\n",
    "Data. The queries were constructed based on user comments from 9 videos in the Stanford CS230: Deep Learning (Autumn 2025), \n",
    "Methodology. The study uses a set of 80 queries representing diverse tasks such as: Information extraction, Sentiment analysis, Topic identification, Inference, Summarization and judgment, etc.\n",
    "The three LLMs being compared are:\n",
    "ChatGPT 5.1, Like K2, and Grok 4.1 (however, to ensure fairness, I will hide the names of the responding models and replace them with random A, B, C)\n",
    "Each model's responses are evaluated according to three key criteria:\n",
    "Accuracy - Correctness of information and avoidance of unsupported hallucinations\n",
    "Relevance - Alignment with the query's intent and requirements\n",
    "Coverage - Breadth and completeness in addressing key aspects mentioned or implied\n",
    "✍️ How to Evaluate\n",
    "Each response will be evaluated using two methods:\n",
    "Pairwise Comparison: Choose the better response between two models\n",
    "Pointwise scoring: Rate each model independently on a scale from 1 to 10\n",
    "Now I will provide comments on each of the 9 videos. You can start when you have commented on all 9 and I signal you to rate them.\n",
    "—--------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now let's begin the scoring. Based on the detailed data from the 9 videos I've provided, please act as the judge and score the following 12 questions:\n",
    "Part 1: Pair Matching - Choose the better response between two models\n",
    "Question {number}: {question_text}\n",
    "Answer A [{answer_a}];\n",
    "Answer B [{answer_b}];\n",
    "Answer C:[{answer_c}].\n",
    "Accuracy\n",
    "Accuracy assesses the extent to which answers accurately reflect the information contained in the comments, avoiding false information, unfounded speculation, and fabricated content not supported by comment data.\n",
    "1. Based on the two answers to question {number}, if considering accuracy alone, which answer, A or B, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
    "☐ A \n",
    "☐ B \n",
    "☐ A and B are tied\n",
    "2. Based on the two answers to question {number}, if considering accuracy alone, which answer, A or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
    "☐ A \n",
    "☐ C \n",
    "☐ A and C are tied\n",
    "3. Based on the two answers to question {number}, considering accuracy alone, which answer, B or C, more accurately reflects the information in the comments, with fewer errors or unfounded speculations?\n",
    "☐ B \n",
    "☐ C\n",
    "☐ B and C are tied\n",
    "Relevance\n",
    "Relevance assesses the degree to which an answer closely addresses the query's requirements. An answer is considered highly relevant if it directly addresses the core question, is concise, doesn't stray from the topic, and doesn't omit any key elements of the query.\n",
    "Based on the two answers to question {number}, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
    "4. Based on the two answers to question {number}, considering only their relevance, which answer, A or B, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
    "☐ A \n",
    "☐ B \n",
    "☐ A and B are tied\n",
    "5. Based on the two answers to question {number}, considering only their relevance, which answer, A or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
    "☐ A \n",
    "☐ C\n",
    "☐ A and C are tied\n",
    "6. Based on the two answers to question {number}, considering only their relevance, which answer, B or C, addresses the core of the query more accurately, avoiding digressions or omissions of the main point?\n",
    "☐ B \n",
    "☐ C\n",
    "☐ B and C are tied\n",
    "3. Coverage\n",
    "Coverage assesses the extent to which an answer fully addresses the important aspects requested in the query. A highly comprehensive answer will synthesize multiple dimensions of information contained in the comments, without omitting any key points or necessary aspects.\n",
    "Based on the two answers to question {number}, considering only their relevance, which answer more accurately addresses the core of the query, avoiding digressions or omissions of the main point?\n",
    "7. Based on the two answers to question {number}, considering only the coverage, which answer, A or B, provides a more complete picture of the key aspects of the query?\n",
    "☐ A \n",
    "☐ B \n",
    "☐ A and B are tied\n",
    "8. Based on the two answers to question {number}, considering only the coverage, which answer, A or C, provides a more complete picture of the key aspects of the query?\n",
    "☐ A \n",
    "☐ C\n",
    "☐ A and C are tied\n",
    "9. Based on the two answers to question {number}, considering only the coverage, which answer, B or C, provides a more complete picture of the key aspects of the query?\n",
    "☐ B \n",
    "☐ C\n",
    "☐ B and C are tied\n",
    "Part 2: Scoring on a 10-point scale\n",
    "Accuracy\n",
    "10. To what extent do the answers accurately reflect the information contained in the comments, and do they avoid misinformation or unfounded speculation?\n",
    "Score, Description of criteria\n",
    "1 – 2\n",
    "Serious discrepancies:The answer contains largely false, fabricated (hallucination), or directly contradictory information that contradicts the input data. It is harmful to or misleads the user.\n",
    "3 – 4\n",
    "Weak:While some information is correct, it is mixed with a lot of misinformation or unfounded speculation. Users cannot trust this answer without verifying it.\n",
    "5 – 6\n",
    "Medium:The basic information is correct, but there are some minor errors in figures, proper names, or supplementary details. There are no serious errors in terms of logic/background knowledge.\n",
    "7 – 8\n",
    "Good:The information is accurate and reliable. There are no factual errors. Any inferences (if any) are logically based on the data.\n",
    "9 – 10\n",
    "Excellent:Absolutely 100% accurate. Every statement is true. There is no ambiguity whatsoever regarding its correctness.\n",
    "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
    "Relevance\n",
    "11. To what extent does the answer closely address the query's requirements? Does it stay focused, avoiding rambling or omitting key points?\n",
    "Score, Description of criteria\n",
    "1 – 2\n",
    "Digress:The answer is irrelevant to the question or addresses a different question altogether. Completely useless.\n",
    "3 – 4\n",
    "Weak:The topic is mentioned but the response is roundabout, contains too much unnecessary information (filler words), or repeats the question verbatim without addressing the issue.\n",
    "5 – 6\n",
    "Medium:The answers are reasonably focused, but there are still rambling passages or disorganized sentence structures that make it difficult for the reader to grasp the main points.\n",
    "7 – 8\n",
    "Good:It directly answers the question. The structure is clear and easy to understand. It eliminates most of the distracting information.\n",
    "9 – 10\n",
    "Excellent:The answer is concise, brief, yet valuable. It gets straight to the point from the first sentence. The formatting (bullet point, bold/light) is optimized for readability.\n",
    "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
    "Relevance\n",
    "12. To what extent does the answer address all the important aspects raised in the query? Are there any significant points omitted?\n",
    "Score, Description of criteria\n",
    "1 – 2\n",
    "Very flawed:Ignoring most of the main requirements of the question. Only answering a very small or insignificant part of it.\n",
    "3 – 4\n",
    "Lack:Omitting at least one important/core aspect of the question. For example: Asking about advantages and disadvantages but only mentioning the advantages.\n",
    "5 – 6\n",
    "Medium:The main points are mentioned, but the discussion lacks depth and is superficial. It is missing necessary supporting details or illustrative examples.\n",
    "7 – 8\n",
    "Good:The response addresses all aspects of the query, leaving no key points unanswered. The depth of the response is good.\n",
    "9 – 10\n",
    "Excellent:Comprehensive and thorough. It not only answers all the questions but also provides context, insightful perspectives, or exceptions (if needed). Exceeds expectations in terms of detail.\n",
    "Please assign points in order: How many points for A, how many points for B, and how many points for C?\n",
    "Please help me answer the following 12 questions.\"\"\"\n",
    "    \n",
    "def gemini_judge_single(row, index, youtube_data=None, final_json_path=FINAL_RESULTS_JSON):\n",
    "    \"\"\"Xử lý 1 hàng CSV -> build prompt -> call_gemini -> save kết quả\"\"\"\n",
    "    number = _get_field(row, ['number', 'Number', 'No', 'no']) or str(index)\n",
    "    question_text = _get_field(row, ['Question (English)', 'Question', 'question', 'Question_English'])\n",
    "    answer_a = _get_field(row, ['GPT 5.1', 'GPT5.1', 'GPT_5.1', 'GPT 5.1 '])\n",
    "    answer_b = _get_field(row, ['Grok 4.1', 'Grok4.1', 'Grok_4.1', 'Grok 4.1 '])\n",
    "    answer_c = _get_field(row, ['Kimi K2', 'KimiK2', 'Kimi_K2', 'Kimi K2 ' , 'Kimi'])\n",
    "\n",
    "    answer_a = answer_a or \"\"\n",
    "    answer_b = answer_b or \"\"\n",
    "    answer_c = answer_c or \"\"\n",
    "    question_text = question_text or \"\"\n",
    "\n",
    "    judge_prompt = build_judge_prompt(number, question_text, answer_a, answer_b, answer_c)\n",
    "    print(judge_prompt)\n",
    "\n",
    "    try:\n",
    "        raw = call_gemini(judge_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] call_gemini failed for row {index} (question {number}): {e}\")\n",
    "        raw = f\"CALL_GEMINI_ERROR: {e}\"\n",
    "\n",
    "    clean_result = clean_text(raw) if raw else \"\"\n",
    "    judgement_lines = clean_result.splitlines() if clean_result else []\n",
    "\n",
    "    item = {\n",
    "        \"question_number\": number,\n",
    "        \"question\": question_text,\n",
    "        \"answers\": {\n",
    "            \"A\": answer_a,\n",
    "            \"B\": answer_b,\n",
    "            \"C\": answer_c\n",
    "        },\n",
    "        \"raw_judge\": raw,\n",
    "        \"clean_judge_lines\": judgement_lines\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        append_json(final_json_path, item)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to append result for question {number}: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "    return item\n",
    "\n",
    "def gemini_judge_all(csv_path=CSV_PATH, youtube_data=None, final_json_path=FINAL_RESULTS_JSON):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "\n",
    "    with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        rows = list(reader)\n",
    "\n",
    "    total = len(rows)\n",
    "    if total == 0:\n",
    "        print(\"CSV empty - no rows to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {total} rows from {csv_path}. Starting to call gemini for each row...\")\n",
    "\n",
    "    results = []\n",
    "    rows = rows[0:2]  # For testing, comment out to process all rows\n",
    "    for i, row in enumerate(rows, start=1):\n",
    "        print(f\"Processing ({i}/{total}) ...\")\n",
    "        item = gemini_judge_single(row, i, youtube_data=youtube_data, final_json_path=final_json_path)\n",
    "        results.append(item)\n",
    "    print(f\"Done. Processed {len(results)} rows. Results saved to {final_json_path}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    youtube_data = None\n",
    "    gemini_judge_all(csv_path=CSV_PATH, youtube_data=youtube_data, final_json_path=FINAL_RESULTS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822b2e2",
   "metadata": {
    "id": "3822b2e2"
   },
   "outputs": [],
   "source": [
    "# def export_json_to_excel(json_file, excel_file):\n",
    "#     if not os.path.exists(json_file):\n",
    "#         print(f\"Error: File not found {json_file}\")\n",
    "#         return\n",
    "\n",
    "#     with open(json_file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     df = pd.DataFrame(data)\n",
    "#     with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "#         df.to_excel(writer, index=False, sheet_name='Compare Results')\n",
    "\n",
    "#         workbook  = writer.book\n",
    "#         worksheet = writer.sheets['Compare Results']\n",
    "\n",
    "#         wrap_format = workbook.add_format({\n",
    "#             'text_wrap': True,\n",
    "#             'valign': 'top',\n",
    "#             'border': 1\n",
    "#         })\n",
    "\n",
    "#         worksheet.set_column('A:A', 60, wrap_format) # Cột Câu hỏi\n",
    "#         worksheet.set_column('B:C', 60, wrap_format) # Cột GPT & Grok Answer\n",
    "#         worksheet.set_column('D:D', 20, wrap_format) # Cột Timestamp\n",
    "#         worksheet.set_column('E:E', 60, wrap_format) # Cột Gemini Judgment\n",
    "# if __name__ == \"__main__\":\n",
    "#     export_json_to_excel(\"gemini_judge.json\", \"AI_Judge.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
